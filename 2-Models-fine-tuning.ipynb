{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46641326-1e71-4d7a-a13b-e3584ab5cb1f",
   "metadata": {
    "id": "46641326-1e71-4d7a-a13b-e3584ab5cb1f"
   },
   "source": [
    "# Model fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb7fd0d-9d7e-43be-9257-644125af64e9",
   "metadata": {
    "id": "3eb7fd0d-9d7e-43be-9257-644125af64e9"
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9164a8d-2e16-45c1-b052-79ff7da7e8cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Setup project paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b1d6c59d2b5ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from os.path import join, exists\n",
    "from py_markdown_table.markdown_table import markdown_table\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# ANSI Highlighting: https://stackoverflow.com/a/21786287\n",
    "h_red = '\\x1b[1;30;41m'\n",
    "h_green = '\\x1b[1;30;42m'\n",
    "h_yellow = '\\x1b[1;30;43m'\n",
    "h_stop = '\\x1b[0m'\n",
    "\n",
    "## Setup project paths:\n",
    "project_path = os.getcwd()\n",
    "models_path = join(project_path, \"Models\")\n",
    "\n",
    "datasets_path = join(project_path, \"Source datasets\")\n",
    "multicw_path = join(project_path, 'Final-dataset')\n",
    "multiclaim_path = join(datasets_path, \"MultiClaim\")\n",
    "lesa_dst_dir = join(datasets_path, 'LESA-EACL-2021')\n",
    "\n",
    "sys.path.insert(1, join('Tools'))\n",
    "from preprocess import to_structured\n",
    "from data_filtration import filter_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb31e5eb4ab1684",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1654cc3-e794-4e5a-9674-a9b962b967e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### MultiCW dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585cff8b46c36f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MultiCW model\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "languages = pd.read_csv(join('Final-dataset', 'multicw.csv'))['lang'].unique()\n",
    "\n",
    "multicw_path = join(\"Final-dataset\")\n",
    "multicw_train = pd.read_csv(join(multicw_path, \"multicw-train.csv\")).astype({'label':'int'})\n",
    "multicw_dev = pd.read_csv(join(multicw_path, \"multicw-dev.csv\")).astype({'label':'int'})\n",
    "multicw_test = pd.read_csv(join(multicw_path, \"multicw-test.csv\")).astype({'label':'int'})\n",
    "\n",
    "multicw_train = multicw_train[['text', 'label']]\n",
    "multicw_dev = multicw_dev[['text', 'label']]\n",
    "multicw_test = multicw_test[['text', 'label', 'style']]\n",
    "\n",
    "multicw_train.dropna(inplace=True)\n",
    "multicw_dev.dropna(inplace=True)\n",
    "multicw_test.dropna(inplace=True)\n",
    "\n",
    "print(multicw_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cf9cc775a89f03",
   "metadata": {},
   "source": [
    "### MultiCW dataset - preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d0abba732ff462",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preprocessing MultiCW train set')\n",
    "multicw_train_norm = to_structured(multicw_train['text'])\n",
    "print('Preprocessing MultiCW validation set')\n",
    "multicw_dev_norm = to_structured(multicw_dev['text'])\n",
    "print('Preprocessing MultiCW test set')\n",
    "multicw_test_norm = to_structured(multicw_test['text'])\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4d6bbc9f4eeab0",
   "metadata": {},
   "source": [
    "### Manually obtained samples from AFP-fact-checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d602a9c982e1620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create an empty DataFrame with specified columns\n",
    "manual_test = pd.DataFrame(columns=[\"text\", \"label\", \"url\"])\n",
    "\n",
    "# Add rows using loc\n",
    "manual_test.loc[0] = [\"Example text 1\", \"positive\", \"http://example.com/1\"]\n",
    "manual_test.loc[1] = [\"Example text 2\", \"negative\", \"http://example.com/2\"]\n",
    "manual_test.loc[2] = [\"Example text 3\", \"neutral\", \"http://example.com/3\"]\n",
    "\n",
    "print('Preprocessing Manually obtained samples:')\n",
    "manual_test['text'] = to_structured(manual_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07373bf5-a7a4-4a8f-a93a-d1054d4b07cf",
   "metadata": {
    "id": "07373bf5-a7a4-4a8f-a93a-d1054d4b07cf",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c090cbc1-47c5-4273-81de-eae6eabcb7b0",
   "metadata": {},
   "source": [
    "### LESA model (EACL-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7c3171-8222-4e7a-8612-05eb0184a180",
   "metadata": {
    "id": "2b7c3171-8222-4e7a-8612-05eb0184a180",
    "jupyter": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from typing import Callable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from pandas import DataFrame\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "\n",
    "sys.path.insert(1, lesa_model_path)\n",
    "\n",
    "from loader import sent2feature2ngram, ParentPositions, tokenize_sentences, create_attention_masks, \\\n",
    "    load_embedding_matrix, load_tokenizer, ind_model_noisy, ind_model_semi, ind_model_structured, final\n",
    "\n",
    "# ANSI Highlighting: https://stackoverflow.com/a/21786287\n",
    "h_stop = '\\x1b[0m'\n",
    "gh_start = '\\x1b[1;30;42m'\n",
    "rh_start = '\\x1b[1;30;41m'\n",
    "\n",
    "\n",
    "class LESAClaimModel():\n",
    "    \"\"\" LESA: Linguistic Encapsulation and Semantic Amalgamation Based Generalised Claim Detection from Online\n",
    "    Content accepted at EACL 2021. ArXiv paper [link](https://arxiv.org/abs/2101.11891) \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.max_len = 30\n",
    "        self.batch_size = 32\n",
    "\n",
    "        self.final_model = None\n",
    "        self.noisy_model = None\n",
    "        self.semi_model = None\n",
    "        self.structured_model = None\n",
    "\n",
    "        self.noisy = None\n",
    "        self.noisy_dev = None\n",
    "        self.semi = None\n",
    "        self.semi_dev = None\n",
    "        self.struct = None\n",
    "        self.struct_dev = None\n",
    "\n",
    "        self.update_semantic_dbs = False\n",
    "\n",
    "        # Set default semantic embedding datasets\n",
    "        # self.set_semantic_datasets()\n",
    "\n",
    "        # Init tokenizers as class variables\n",
    "        print('Loading tokenizers: ', end='')\n",
    "\n",
    "        # Original BERT model\n",
    "        # model = os.path.join(lesa_model_path, 'bert-base')\n",
    "        # self.bert_tokenizer_transformer = BertTokenizer.from_pretrained(model, local_files_only=True)\n",
    "\n",
    "        # Multilingual BERT model\n",
    "        model = \"bert-base-multilingual-cased\"\n",
    "        self.bert_tokenizer_transformer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "        # EMBEDDINGS\n",
    "        self.noisy_embedding_matrix_tag = load_embedding_matrix(lesa_model_path, 'embedding_matrix_tag_noisy.pickle')\n",
    "        self.noisy_vocab_size_tag = self.noisy_embedding_matrix_tag.shape[0]  # 5363\n",
    "\n",
    "        self.semi_embedding_matrix_tag = load_embedding_matrix(lesa_model_path, 'embedding_matrix_tag_semi.pickle')\n",
    "        self.semi_vocab_size_tag = self.semi_embedding_matrix_tag.shape[0]  # 6137\n",
    "\n",
    "        self.structured_embedding_matrix_tag = load_embedding_matrix(lesa_model_path, 'embedding_matrix_tag_structured.pickle')\n",
    "        self.structured_vocab_size_tag = self.structured_embedding_matrix_tag.shape[0]  # 6048\n",
    "\n",
    "        # PARENT POS TOKENIZER\n",
    "        self.tokenizer_dep_parent_noisy = load_tokenizer(lesa_model_path, 'tokenizer_dep_parent_noisy.pickle')\n",
    "        self.num_words_dep_parent_noisy = self.tokenizer_dep_parent_noisy.num_words  # 100\n",
    "\n",
    "        self.tokenizer_dep_parent_semi = load_tokenizer(lesa_model_path, 'tokenizer_dep_parent_semi.pickle')\n",
    "        self.num_words_dep_parent_semi = self.tokenizer_dep_parent_semi.num_words  # 200\n",
    "\n",
    "        self.tokenizer_dep_parent_structured = load_tokenizer(lesa_model_path, 'tokenizer_dep_parent_structured.pickle')\n",
    "        self.num_words_dep_parent_structured = self.tokenizer_dep_parent_structured.num_words  # 200\n",
    "\n",
    "        # LABEL TOKENIZER\n",
    "        self.tokenizer_dep_noisy = load_tokenizer(lesa_model_path, 'tokenizer_dep_noisy.pickle')\n",
    "        self.num_words_dep_noisy = self.tokenizer_dep_noisy.num_words  # 6300\n",
    "\n",
    "        self.tokenizer_dep_semi = load_tokenizer(lesa_model_path, 'tokenizer_dep_semi.pickle')\n",
    "        self.num_words_dep_semi = self.tokenizer_dep_semi.num_words  # 7300\n",
    "\n",
    "        self.tokenizer_dep_structured = load_tokenizer(lesa_model_path, 'tokenizer_dep_structured.pickle')\n",
    "        self.num_words_dep_structured = self.tokenizer_dep_structured.num_words  # 7400\n",
    "\n",
    "        # TAG TOKENIZER\n",
    "        self.tokenizer_tag_noisy = load_tokenizer(lesa_model_path, 'tokenizer_tag_noisy.pickle')\n",
    "        self.tokenizer_tag_semi = load_tokenizer(lesa_model_path, 'tokenizer_tag_semi.pickle')\n",
    "        self.tokenizer_tag_structured = load_tokenizer(lesa_model_path, 'tokenizer_tag_structured.pickle')\n",
    "        print('ok')\n",
    "\n",
    "        self.final_model = self._init_model()\n",
    "\n",
    "    def load_model(self, model_name='lesa2021') -> bool:\n",
    "        \"\"\"Loads the model from the file.\"\"\"\n",
    "\n",
    "        os.makedirs(models_path, exist_ok=True)\n",
    "\n",
    "        print(join(os.getcwd(), models_path, model_name))\n",
    "        if not os.path.exists(join(models_path, model_name)):\n",
    "            print(rh_start + 'Invalid path!' + h_stop)\n",
    "            return False\n",
    "\n",
    "        print(f'Loading {model_name} model: ', end='')\n",
    "        try:\n",
    "            self.noisy_model.load_weights(join(models_path, model_name, '_dep_noisy.h5'))\n",
    "            self.semi_model.load_weights(join(models_path, model_name, '_dep_semi.h5'))\n",
    "            self.structured_model.load_weights(join(models_path, model_name, '_dep_structured.h5'))\n",
    "\n",
    "            self.final_model.load_weights(join(models_path, model_name, '_bert_comb.h5'))\n",
    "\n",
    "            print(gh_start + \" ok\" + h_stop)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            # print(e)\n",
    "            print(rh_start + \" failed!\" + h_stop)\n",
    "            return False\n",
    "\n",
    "    def detect_claims(self, test_set: DataFrame, verbose=False) -> tuple:\n",
    "        \"\"\"Performs inference on the testing data and evaluates results.\"\"\"\n",
    "\n",
    "        x = self.semantic_embeddings(test_set)\n",
    "\n",
    "        # testing\n",
    "        print('Running classification:')\n",
    "        metrics = self.final_model.predict(x)\n",
    "        results = [np.argmax(el) for el in metrics[-1]]\n",
    "        print('Done.')\n",
    "        # Print sentences with classifications\n",
    "        if verbose:\n",
    "            for i, text in enumerate(test_set['text']):\n",
    "                print(\"LESA classification: '{text}' {c} a claim.\".\n",
    "                      format(text=text, c=gh_start + \"is\" if results[i] == 1 else rh_start + \"is not\") + h_stop)\n",
    "\n",
    "        # compare against ground-truth\n",
    "        ground_truth = test_set['label']\n",
    "        report = classification_report(ground_truth, results, output_dict=True)\n",
    "        report_str = str(classification_report(ground_truth, results))\n",
    "\n",
    "        return report, report_str\n",
    "\n",
    "    def train_model(self, train_set: DataFrame, dev_set: DataFrame, epochs=1, learn_rate=3e-5, model_name='', lang='en'):\n",
    "        \"\"\"\n",
    "        Train the LESA-2021 model with the given parameters. The training consists of two phases:\n",
    "        - Pre-training of semantic modules with noisy, semi-noisy and structured data respectively\n",
    "        - Fine-tuning of the main BERT model together with the semantic model on the training data\n",
    "        :param learn_rate: Learning rate.\n",
    "        :param train_set: Training dataset.\n",
    "        :param dev_set: Validation dataset.\n",
    "        :param epochs: Number of training epochs.\n",
    "        :param model_name: Model will be saved to the directory named by this value.If left blank, the model won't save.\n",
    "        :param lang: Training dataset language(s). Needed for naming conventions.\n",
    "        :return: Trained model.\n",
    "        \"\"\"\n",
    "\n",
    "        model_name = f\"{model_name}-{lang}-{epochs}e\"\n",
    "        path = os.path.join(models_path, model_name)\n",
    "        semantic_path = os.path.join(lesa_model_path, 'semantic_base')\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        if not self.update_semantic_dbs:\n",
    "            # Make sure that semantic models are loaded\n",
    "            shutil.copyfile(src=os.path.join(semantic_path, '_dep_noisy.h5'), dst=os.path.join(path, '_dep_noisy.h5'))\n",
    "            shutil.copyfile(src=os.path.join(semantic_path, '_dep_semi.h5'), dst=os.path.join(path, '_dep_semi.h5'))\n",
    "            shutil.copyfile(src=os.path.join(semantic_path, '_dep_structured.h5'), dst=os.path.join(path, '_dep_structured.h5'))\n",
    "\n",
    "            self.noisy_model.load_weights(os.path.join(semantic_path, '_dep_noisy.h5'))\n",
    "            self.semi_model.load_weights(os.path.join(semantic_path, '_dep_semi.h5'))\n",
    "            self.structured_model.load_weights(os.path.join(semantic_path, '_dep_structured.h5'))\n",
    "        else:\n",
    "            # If we use custom semantic models, make sure they are saved within the same folder as the model file\n",
    "            self.noisy_model.save_weights(os.path.join(path, '_dep_noisy.h5'))\n",
    "            print('Model saved to: ', os.path.join(path, '_dep_noisy.h5'))\n",
    "\n",
    "            self.semi_model.save_weights(os.path.join(path, '_dep_semi.h5'))\n",
    "            print('Model saved to: ', os.path.join(path, '_dep_semi.h5'))\n",
    "\n",
    "            self.structured_model.save_weights(os.path.join(path, '_dep_structured.h5'))\n",
    "            print('Model saved to: ', os.path.join(path, '_dep_structured.h5'))\n",
    "\n",
    "        print('Train Final model:')\n",
    "        x = self.semantic_embeddings(train_set)\n",
    "        dev = self.semantic_embeddings(dev_set)\n",
    "\n",
    "        self.final_model.optimizer = Adam(learning_rate=learn_rate)\n",
    "        self.final_model.fit(x=x, y=train_set['label'], batch_size=self.batch_size, epochs=epochs,\n",
    "                             validation_data=(dev, dev_set['label']))\n",
    "        self.final_model.save_weights(os.path.join(path, '_bert_comb.h5'))\n",
    "        print('Model saved to: ', os.path.join(path, '_bert_comb.h5'))\n",
    "\n",
    "    def _init_model(self):\n",
    "        # load_model\n",
    "        print(\"Initializing model architecture: \")\n",
    "\n",
    "        # aux CLAIMS-2023 model\n",
    "        self.noisy_model = ind_model_noisy(embed_dim=20, num_heads=5, ff_dim=128,\n",
    "                                           maxlen=self.max_len, vocab_label=self.num_words_dep_noisy,\n",
    "                                           vocab_parent_pos=self.num_words_dep_parent_noisy)\n",
    "\n",
    "        self.semi_model = ind_model_semi(embed_dim=20, num_heads=5, ff_dim=128,\n",
    "                                         maxlen=self.max_len, vocab_label=self.num_words_dep_semi,\n",
    "                                         vocab_parent_pos=self.num_words_dep_parent_semi)\n",
    "\n",
    "        self.structured_model = ind_model_structured(embed_dim=20, num_heads=5, ff_dim=128,\n",
    "                                                     maxlen=self.max_len, vocab_label=self.num_words_dep_structured,\n",
    "                                                     vocab_parent_pos=self.num_words_dep_parent_structured)\n",
    "\n",
    "        parameters_dict_noisy = {\n",
    "            \"vocab_size_tag\": self.noisy_vocab_size_tag,\n",
    "            \"EMBEDDING_DIM_TAG\": 20,\n",
    "            \"embedding_matrix_tag\": self.noisy_embedding_matrix_tag,\n",
    "            \"maxlen_tag\": self.max_len\n",
    "        }\n",
    "\n",
    "        parameters_dict_semi = {\n",
    "            \"vocab_size_tag\": self.semi_vocab_size_tag,\n",
    "            \"EMBEDDING_DIM_TAG\": 20,\n",
    "            \"embedding_matrix_tag\": self.semi_embedding_matrix_tag,\n",
    "            \"maxlen_tag\": self.max_len\n",
    "        }\n",
    "\n",
    "        parameters_dict_structured = {\n",
    "            \"vocab_size_tag\": self.structured_vocab_size_tag,\n",
    "            \"EMBEDDING_DIM_TAG\": 20,\n",
    "            \"embedding_matrix_tag\": self.structured_embedding_matrix_tag,\n",
    "            \"maxlen_tag\": self.max_len\n",
    "        }\n",
    "\n",
    "        final_model = final(lesa_model_path, self.noisy_model, self.semi_model, self.structured_model, parameters_dict_noisy,\n",
    "                            parameters_dict_semi, parameters_dict_structured, max_seq_length=60)\n",
    "\n",
    "        print('ok')\n",
    "\n",
    "        return final_model\n",
    "\n",
    "    def semantic_embeddings(self, dataset: DataFrame):\n",
    "        # GET SYNTACTIC REP: TEST\n",
    "        # Assuming 'original_dataset' is your original DataFrame\n",
    "        dataset = dataset.copy()\n",
    "\n",
    "        print(\"Getting dependency and POS tags...\")\n",
    "\n",
    "        # Processing DEP tags\n",
    "        progress = tqdm(dataset['text'].copy())\n",
    "        progress.set_description('sentence2pos tags:')\n",
    "        dataset.loc[:, 'DEP'] = [sent2feature2ngram(row) for row in progress]\n",
    "\n",
    "        # Processing parent POS tags\n",
    "        progress = tqdm(dataset['text'].copy())\n",
    "        progress.set_description('Parent POS tags:')\n",
    "        dataset.loc[:, \"parent_pos\"] = [ParentPositions(row) for row in progress]\n",
    "\n",
    "        # Processing TAG tags\n",
    "        progress = tqdm(dataset['text'].copy())\n",
    "        progress.set_description('sentence2tag tags:')\n",
    "        dataset.loc[:, 'TAG'] = [sent2feature2ngram(row, feature=\"TAG\") for row in progress]\n",
    "\n",
    "        print(\"POS Tags: complete!\")\n",
    "\n",
    "        # COMMON TEST REP | DEP\n",
    "        print(\"Getting dependency and DEP tags...\")\n",
    "        noisy = self.tokenizer_dep_noisy.texts_to_sequences(dataset['DEP'])\n",
    "        semi = self.tokenizer_dep_semi.texts_to_sequences(dataset['DEP'])\n",
    "        structured = self.tokenizer_dep_structured.texts_to_sequences(dataset['DEP'])\n",
    "\n",
    "        p_noisy = self.tokenizer_dep_parent_noisy.texts_to_sequences(dataset['parent_pos'])\n",
    "        p_semi = self.tokenizer_dep_parent_semi.texts_to_sequences(dataset['parent_pos'])\n",
    "        p_structured = self.tokenizer_dep_parent_structured.texts_to_sequences(dataset['parent_pos'])\n",
    "\n",
    "        noisy = pad_sequences(noisy, maxlen=self.max_len)\n",
    "        semi = pad_sequences(semi, maxlen=self.max_len)\n",
    "        structured = pad_sequences(structured, maxlen=self.max_len)\n",
    "\n",
    "        p_noisy = pad_sequences(p_noisy, maxlen=self.max_len)\n",
    "        p_semi = pad_sequences(p_semi, maxlen=self.max_len)\n",
    "        p_structured = pad_sequences(p_structured, maxlen=self.max_len)\n",
    "\n",
    "        print(\"DEP Tags: complete!\")\n",
    "\n",
    "        # BERT REP: TEST\n",
    "        print(\"Creating BERT Embeddings...\")\n",
    "        input_ids = tokenize_sentences(dataset['text'], self.bert_tokenizer_transformer, 60)\n",
    "        input_ids = pad_sequences(input_ids, maxlen=60, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "        attention_masks = create_attention_masks(input_ids)\n",
    "\n",
    "        # COMMON TEST REP | TAG\n",
    "        tag_noisy = self.tokenizer_tag_noisy.texts_to_sequences(dataset['TAG'])\n",
    "        tag_semi = self.tokenizer_tag_semi.texts_to_sequences(dataset['TAG'])\n",
    "        tag_structured = self.tokenizer_tag_structured.texts_to_sequences(dataset['TAG'])\n",
    "\n",
    "        noisy[noisy >= self.noisy_vocab_size_tag] = self.noisy_vocab_size_tag - 1\n",
    "        p_noisy[p_noisy >= self.noisy_vocab_size_tag] = self.noisy_vocab_size_tag - 1\n",
    "        tag_noisy = np.array([np.array(x) for x in tqdm(tag_noisy)], dtype=object)\n",
    "        for arrays in tag_noisy:\n",
    "            arrays[arrays >= self.noisy_vocab_size_tag] = self.noisy_vocab_size_tag - 1\n",
    "\n",
    "        tag_noisy = pad_sequences(tag_noisy, maxlen=self.max_len)\n",
    "        tag_semi = pad_sequences(tag_semi, maxlen=self.max_len)\n",
    "        tag_structured = pad_sequences(tag_structured, maxlen=self.max_len)\n",
    "\n",
    "        x = {\"label_noisy\": np.array(noisy), \"parent_pos_noisy\": np.array(p_noisy),\n",
    "             \"label_semi\": np.array(semi), \"parent_pos_semi\": np.array(p_semi),\n",
    "             \"label_structured\": np.array(structured),\n",
    "             \"parent_pos_structured\": np.array(p_structured),\n",
    "             \"inp_noisy\": np.array(tag_noisy),\n",
    "             \"inp_semi\": np.array(tag_semi),\n",
    "             \"inp_structured\": np.array(tag_structured),\n",
    "             'input_word_ids': np.array(input_ids), 'input_masks': np.array(attention_masks)}\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e120cb5dc7290406",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### XLM-RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l2-2eoLXjJKJ",
   "metadata": {
    "id": "l2-2eoLXjJKJ"
   },
   "outputs": [],
   "source": [
    "import keras_hub\n",
    "import keras\n",
    "preprocessor = keras_hub.models.TextClassifierPreprocessor.from_preset(\n",
    "    \"xlm_roberta_base_multi\",\n",
    "    sequence_length=256 # Optional.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1nnytAhaNAny",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1nnytAhaNAny",
    "outputId": "8cddf7b0-303c-4de4-d926-24ecd5edbd25"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from typing import Callable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras_hub\n",
    "import keras\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from pandas import DataFrame\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "\n",
    "\n",
    "class XLMRobertaModel():\n",
    "    \"\"\" Model finetuning and inference on the LESA dataset using XLMRoberta \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.max_len = 256\n",
    "        self.batch_size = 32\n",
    "\n",
    "        self.final_model = None\n",
    "\n",
    "        self.preprocessor = keras_hub.models.TextClassifierPreprocessor.from_preset(\n",
    "            'xlm_roberta_base_multi',\n",
    "            sequence_length=self.max_len\n",
    "        )\n",
    "\n",
    "    def load_model(self, model_name) -> bool:\n",
    "        try:\n",
    "            # 1. Instantiate the model with the known preset (architecture)\n",
    "            self.final_model = keras_hub.models.XLMRobertaTextClassifier.from_preset(\n",
    "                'xlm_roberta_base_multi',\n",
    "                num_classes=2,\n",
    "                preprocessor=self.preprocessor,\n",
    "                dropout=0.2\n",
    "            )\n",
    "    \n",
    "            # 2. Load the weights manually from the path\n",
    "            weights_path = os.path.join('Models', f'{model_name}.weights.h5')\n",
    "            self.final_model.load_weights(weights_path)\n",
    "    \n",
    "            print('Model loaded successfully.')\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def detect_claims(self, test_set: DataFrame, verbose=False) -> tuple:\n",
    "        \"\"\"Performs inference on the testing data and evaluates results.\"\"\"\n",
    "\n",
    "        print('Running classification:')\n",
    "        metrics = self.final_model.predict(x=test_set['text'].to_numpy(), batch_size=self.batch_size)\n",
    "\n",
    "        # Testing\n",
    "        results = np.argmax(metrics, axis=1)\n",
    "        print('Done.')\n",
    "        # Print sentences with classifications\n",
    "        if verbose:\n",
    "            for i, text in enumerate(test_set['text']):\n",
    "                print(\"LESA classification: '{text}' {c} a claim.\".\n",
    "                      format(text=text, c=gh_start + \"is\" if results[i] == 1 else rh_start + \"is not\") + h_stop)\n",
    "\n",
    "        # Compare against ground-truth\n",
    "        ground_truth = test_set['label'].to_numpy()\n",
    "        report = classification_report(ground_truth, results, output_dict=True)\n",
    "        report_str = str(classification_report(ground_truth, results))\n",
    "\n",
    "        return report, report_str\n",
    "\n",
    "    def train_model(self, train_set: DataFrame, dev_set: DataFrame, epochs=1, learn_rate=3e-5, model_name='', lang='en'):\n",
    "        \"\"\"\n",
    "        Train the XLMRoberta model with the given parameters.\n",
    "\n",
    "        :param learn_rate: Learning rate.\n",
    "        :param train_set: Training dataset.\n",
    "        :param dev_set: Validation dataset.\n",
    "        :param epochs: Number of training epochs.\n",
    "        :param model_name: Model will be saved to the directory named by this value.If left blank, the model won't save.\n",
    "        :param lang: Training dataset language(s). Needed for naming conventions.\n",
    "        :return: Trained model.\n",
    "        \"\"\"\n",
    "\n",
    "        model_name_path = f\"{model_name}-{lang}-{epochs}e\"\n",
    "        path = os.path.join('Models', 'LESA', 'models', model_name_path)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "\n",
    "        self.final_model = keras_hub.models.XLMRobertaTextClassifier.from_preset(\n",
    "            'xlm_roberta_base_multi',\n",
    "            num_classes=len(train_set['label'].unique()),\n",
    "            preprocessor=self.preprocessor,\n",
    "            dropout=0.2\n",
    "        )\n",
    "\n",
    "        for layer in self.final_model.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "        self.final_model.compile(\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            optimizer=keras.optimizers.Adam(learn_rate),\n",
    "            jit_compile=True,\n",
    "        )\n",
    "\n",
    "        self.final_model.fit(x=train_set['text'].to_numpy(), y=train_set['label'].to_numpy(), batch_size=self.batch_size,\n",
    "                             epochs=epochs, validation_data=(dev_set['text'].to_numpy(), dev_set['label'].to_numpy()))\n",
    "\n",
    "        model_save_path = os.path.join(path, 'model.keras')\n",
    "        self.final_model.save(model_save_path)\n",
    "\n",
    "    def inference(self, texts: list[str]) -> list[bool]:\n",
    "        \"\"\"Returns a list of booleans: True if classified as a claim (class 1), else False.\"\"\"\n",
    "        if not isinstance(texts, list):\n",
    "            raise ValueError(\"Input must be a list of strings.\")\n",
    "        if not all(isinstance(t, str) for t in texts):\n",
    "            raise ValueError(\"All items in input list must be strings.\")\n",
    "    \n",
    "        predictions = self.final_model.predict(texts)  # Must be list[str]\n",
    "        classifications = np.argmax(predictions, axis=1)  # shape (batch_size,)\n",
    "        \n",
    "        return (classifications == 1).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb81e0d1219ede9",
   "metadata": {},
   "source": [
    "### mDeBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KsQt7jx907jZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KsQt7jx907jZ",
    "outputId": "f36d7789-b147-479e-951e-b54d68bd6626"
   },
   "outputs": [],
   "source": [
    "import keras_hub\n",
    "import keras\n",
    "preprocessor = keras_hub.models.TextClassifierPreprocessor.from_preset(\n",
    "    \"deberta_v3_base_multi\",\n",
    "    sequence_length=256 # Optional.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o-kYQPkJyLcm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o-kYQPkJyLcm",
    "outputId": "2676b48b-f4ec-415d-e5af-4d60d6d48d01"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from typing import Callable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras_hub\n",
    "import keras\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from pandas import DataFrame\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "\n",
    "\n",
    "class MDeBertaModel():\n",
    "    \"\"\"  \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=''):\n",
    "        self.max_len = 256\n",
    "        self.batch_size = 32\n",
    "        self.final_model = None\n",
    "        \n",
    "        self.preprocessor = keras_hub.models.TextClassifierPreprocessor.from_preset(\n",
    "            'deberta_v3_base_multi',\n",
    "            sequence_length=self.max_len\n",
    "        )\n",
    "\n",
    "    def load_model(self, model_name) -> bool:\n",
    "        try:\n",
    "            model_path = os.path.join('Models', model_name + '.keras')\n",
    "    \n",
    "            if not os.path.exists(model_path):\n",
    "                raise FileNotFoundError(f\"Saved model not found at: {model_path}\")\n",
    "    \n",
    "            print(f\"Loading full model from: {model_path}\")\n",
    "            self.final_model = keras.models.load_model(model_path)\n",
    "            print(\"Model loaded successfully.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {str(e)}\")\n",
    "            return False\n",
    "            \n",
    "    def detect_claims(self, test_set: DataFrame, verbose=False) -> tuple:\n",
    "        \"\"\"Performs inference on the testing data and evaluates results.\"\"\"\n",
    "\n",
    "        print('Running classification:')\n",
    "        metrics = self.final_model.predict(x=test_set['text'].to_numpy(), batch_size=self.batch_size)\n",
    "\n",
    "        # Testing\n",
    "        results = np.argmax(metrics, axis=1)\n",
    "        print('Done.')\n",
    "        # Print sentences with classifications\n",
    "        if verbose:\n",
    "            for i, text in enumerate(test_set['text']):\n",
    "                print(\"LESA classification: '{text}' {c} a claim.\".\n",
    "                      format(text=text, c=gh_start + \"is\" if results[i] == 1 else rh_start + \"is not\") + h_stop)\n",
    "\n",
    "        # Compare against ground-truth\n",
    "        ground_truth = test_set['label'].to_numpy()\n",
    "        report = classification_report(ground_truth, results, output_dict=True)\n",
    "        report_str = str(classification_report(ground_truth, results))\n",
    "\n",
    "        return report, report_str\n",
    "\n",
    "    def train_model(self, train_set: DataFrame, dev_set: DataFrame, epochs=1, learn_rate=3e-5, model_name='', lang='en', final_learn_rate_fraction=0.5):\n",
    "        \"\"\"\n",
    "        Train the mDeBerta model with the given parameters.\n",
    "        :param learn_rate: Learning rate.\n",
    "        :param train_set: Training dataset.\n",
    "        :param dev_set: Validation dataset.\n",
    "        :param epochs: Number of training epochs.\n",
    "        :param model_name: Model will be saved to the directory named by this value.If left blank, the model won't save.\n",
    "        :param lang: Training dataset language(s). Needed for naming conventions.\n",
    "        :return: Trained model.\n",
    "        \"\"\"\n",
    "\n",
    "        self.final_model = keras_hub.models.DebertaV3Classifier.from_preset(\n",
    "            'deberta_v3_base_multi',\n",
    "            num_classes=len(set(train_set['label'].unique())),\n",
    "            preprocessor=self.preprocessor,\n",
    "            dropout=0.2\n",
    "        )\n",
    "\n",
    "        for layer in self.final_model.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "        initial_learning_rate = learn_rate\n",
    "        decay_steps = int(len(train_set) // self.batch_size * epochs)   # Number of steps over which the decay is applied\n",
    "        alpha = final_learn_rate_fraction  # Minimum learning rate as a fraction of initial_learning_rate\n",
    "\n",
    "        lr_schedule = CosineDecay(\n",
    "            initial_learning_rate=initial_learning_rate,\n",
    "            decay_steps=decay_steps,\n",
    "            alpha=alpha\n",
    "        )\n",
    "\n",
    "        self.final_model.compile(\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "            jit_compile=True\n",
    "        )\n",
    "\n",
    "        self.final_model.fit(x=train_set['text'].to_numpy(), y=train_set['label'].to_numpy(), batch_size=self.batch_size,\n",
    "                             epochs=epochs, validation_data=(dev_set['text'].to_numpy(), dev_set['label'].to_numpy()))\n",
    "\n",
    "        model_save_path = os.path.join(path, f\"{model_name}-{lang}-{epochs}e.keras\")\n",
    "        self.final_model.save(model_save_path)\n",
    "        \n",
    "    def inference(self, texts: list[str]) -> list[bool]:\n",
    "        \"\"\"Returns a list of booleans: True if classified as a claim (class 1), else False.\"\"\"\n",
    "        if not isinstance(texts, list):\n",
    "            raise ValueError(\"Input must be a list of strings.\")\n",
    "        if not all(isinstance(t, str) for t in texts):\n",
    "            raise ValueError(\"All items in input list must be strings.\")\n",
    "    \n",
    "        predictions = self.final_model.predict(texts)  # Must be list[str]\n",
    "        classifications = np.argmax(predictions, axis=1)  # shape (batch_size,)\n",
    "        \n",
    "        return (classifications == 1).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5104ea-1327-4fbd-bd6b-edcbec967bd4",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd05912ce3053e",
   "metadata": {},
   "source": [
    "### Models fine-tuning on MultiCW dataset\n",
    "- Fine-tuning of xlm-RoBERTa, mDeBERTa anb LESA models on MultiCW train set\n",
    "- Evaluation on MultiCW test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67e86a4-96fa-4fc9-bc1f-5d7ee253bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "detector = None\n",
    "models = ['xlm', 'mdb', 'lesa']\n",
    "\n",
    "for model in models:\n",
    "    if model == 'xlm':\n",
    "        print(f'{h_green}XLM-RoBERTa model:{h_stop}')\n",
    "        detector = XLMRobertaModel()\n",
    "    if model == 'mdb':\n",
    "        print(f'{h_green}mDBERTa model:{h_stop}')\n",
    "        detector = MDeBertaModel()\n",
    "    if model == 'lesa':\n",
    "        print(f'{h_green}LESA model:{h_stop}')\n",
    "        detector = LESAClaimModel()\n",
    "\n",
    "if not detector.load_model(model_name=f'{model}-multicw-2e6-5e'):\n",
    "    print(f'{h_yellow}No model found. Initiating fine-tuning:{h_stop}')\n",
    "    # Note: Works well with a small learning rate (e.g. 3e-6)\n",
    "    detector.train_model(multicw_train, multicw_dev, epochs=5, learn_rate=2e-6, lang='en', model_name=f'{model}-multicw')\n",
    "\n",
    "print(f'{h_green}MultiCW overall:{h_stop}')\n",
    "_, report = detector.detect_claims(multicw_test)\n",
    "print(report)\n",
    "\n",
    "test_noisy = multicw_test.loc[multicw_test['style']=='noisy']\n",
    "_, report = detector.detect_claims(test_noisy, verbose=False)\n",
    "print(f'{h_yellow}MultiCW Noisy Part:{h_stop}')\n",
    "print(report)\n",
    "\n",
    "test_strut = multicw_test.loc[multicw_test['style']=='struct']\n",
    "_, report = detector.detect_claims(test_strut, verbose=False)\n",
    "print(f'{h_yellow}MultiCW Structured Part:{h_stop}')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994e5b0e-b66c-4f5c-98fb-314064806557",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Fine-tuned models evaluation with preprocessing\n",
    "- Use Qwen2.5-7b model to convert all the samples to a structured writing style before evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb770a44a885a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "detector = None\n",
    "models = ['xlm', 'mdb', 'lesa']\n",
    "\n",
    "for model in models:\n",
    "    if model == 'xlm':\n",
    "        print(f'{h_green}XLM-RoBERTa model:{h_stop}')\n",
    "        detector = XLMRobertaModel()\n",
    "    if model == 'mdb':\n",
    "        print(f'{h_green}mDBERTa model:{h_stop}')\n",
    "        detector = MDeBertaModel()\n",
    "    if model == 'lesa':\n",
    "        print(f'{h_green}LESA model:{h_stop}')\n",
    "        detector = LESAClaimModel()\n",
    "\n",
    "assert detector.load_model(model_name=f'{model}-multicw-2e6-5e'), f'{h_red}No model found!{h_stop}'\n",
    "\n",
    "print(f'{h_green}MultiCW overall:{h_stop}')\n",
    "_, report = detector.detect_claims(multicw_test_norm)\n",
    "print(report)\n",
    "\n",
    "test_noisy = multicw_test_norm.loc[multicw_test_norm['style']=='noisy']\n",
    "_, report = detector.detect_claims(test_noisy, verbose=False)\n",
    "print(f'{h_yellow}MultiCW Noisy Part:{h_stop}')\n",
    "print(report)\n",
    "\n",
    "test_strut = multicw_test_norm.loc[multicw_test_norm['style']=='struct']\n",
    "_, report = detector.detect_claims(test_strut, verbose=False)\n",
    "print(f'{h_yellow}MultiCW Structured Part:{h_stop}')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f43847a8ba1952a",
   "metadata": {},
   "source": [
    "### Out-domain evaluation\n",
    "- Evaluation of the fine-tuned models on manually obtained samples from factcheck.afp.com and with the preprocessing applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e6f104-4696-4390-9d44-b9e40b88c988",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "detector = None\n",
    "models = ['xlm', 'mdb', 'lesa']\n",
    "\n",
    "for model in models:\n",
    "    if model == 'xlm':\n",
    "        print(f'{h_green}XLM-RoBERTa model:{h_stop}')\n",
    "        detector = XLMRobertaModel()\n",
    "    if model == 'mdb':\n",
    "        print(f'{h_green}mDBERTa model:{h_stop}')\n",
    "        detector = MDeBertaModel()\n",
    "    if model == 'lesa':\n",
    "        print(f'{h_green}LESA model:{h_stop}')\n",
    "        detector = LESAClaimModel()\n",
    "\n",
    "assert detector.load_model(model_name=f'{model}-multicw-2e6-5e'), f'{h_red}No model found!{h_stop}'\n",
    "\n",
    "results = detector.inference(manual_test)\n",
    "for text, worthy in zip(manual_test['text'], results):\n",
    "    if worthy:\n",
    "        print(f'{h_green}True{h_stop}: {text}')\n",
    "    else:\n",
    "        print(f'{h_red}False{h_stop}: {text}')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "modelId": 2834,
     "modelInstanceId": 4721,
     "sourceId": 6189,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 2820,
     "modelInstanceId": 4688,
     "sourceId": 6067,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "MultiCW-finetune",
   "language": "python",
   "name": "multicw-finetune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0abbc91f30b04bd3b464ab626caa4ef6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2511f785d020410e851c8f198278c7da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "cpu",
       "gpu"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Storage mode:",
      "description_tooltip": null,
      "disabled": false,
      "index": 1,
      "layout": "IPY_MODEL_8af9da3904434c8db72bf48038475c9e",
      "style": "IPY_MODEL_0abbc91f30b04bd3b464ab626caa4ef6"
     }
    },
    "2c2931510812437899b50874520958b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "353fd4c3ba06412d9431ead32ef52b9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "IntSliderView",
      "continuous_update": true,
      "description": "Batch size:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_8642bd2f2a174d2eafd1ae06aaa494e5",
      "max": 32,
      "min": 1,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": "d",
      "step": 1,
      "style": "IPY_MODEL_6305e85181ee4afa84499a116c911b1c",
      "value": 8
     }
    },
    "5b36326a73a84c80aadbe97684b4083c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bfac425cbea74dbca85bc28aec8fca84",
       "IPY_MODEL_353fd4c3ba06412d9431ead32ef52b9c",
       "IPY_MODEL_e2a27094e6ab478e887d6dbd0d166255",
       "IPY_MODEL_c498aabcc4754ea69cc7939e23b295f2",
       "IPY_MODEL_f0a443c22c71401cadcd0ed9d720a50b",
       "IPY_MODEL_2511f785d020410e851c8f198278c7da"
      ],
      "layout": "IPY_MODEL_f151a00a84a444a2ac3dde95d6449f76"
     }
    },
    "6305e85181ee4afa84499a116c911b1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "",
      "handle_color": null
     }
    },
    "6d2245fa0357414793ec484e42a407ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "702fc0ae0e9646e195974358ada8698e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "",
      "handle_color": null
     }
    },
    "8642bd2f2a174d2eafd1ae06aaa494e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8988b2e3116348faae276ec36bf6b2af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8af9da3904434c8db72bf48038475c9e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bfac425cbea74dbca85bc28aec8fca84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "mDeBERTa",
       "XLM-RoBERTa",
       "LESA"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Model:",
      "description_tooltip": null,
      "disabled": false,
      "index": 0,
      "layout": "IPY_MODEL_ecacd507e05b492dbdcfd924c6c7e039",
      "style": "IPY_MODEL_8988b2e3116348faae276ec36bf6b2af"
     }
    },
    "c1fa2f69f0a147e891184b3d41decae5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "",
      "handle_color": null
     }
    },
    "c498aabcc4754ea69cc7939e23b295f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatLogSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatLogSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "FloatLogSliderView",
      "base": 10,
      "continuous_update": true,
      "description": "Learning Rate:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_6d2245fa0357414793ec484e42a407ce",
      "max": -1,
      "min": -10,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": ".1e",
      "step": 0.1,
      "style": "IPY_MODEL_cf376d4917444367af900a2ea6fbfbd1",
      "value": 3e-05
     }
    },
    "cf376d4917444367af900a2ea6fbfbd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "initial",
      "handle_color": null
     }
    },
    "e2a27094e6ab478e887d6dbd0d166255": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "IntSliderView",
      "continuous_update": true,
      "description": "Batch chunk size:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_fb777cd857e848febd846dabb6f351e4",
      "max": 32,
      "min": 1,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": "d",
      "step": 1,
      "style": "IPY_MODEL_c1fa2f69f0a147e891184b3d41decae5",
      "value": 8
     }
    },
    "ecacd507e05b492dbdcfd924c6c7e039": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0a443c22c71401cadcd0ed9d720a50b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "FloatSliderView",
      "continuous_update": true,
      "description": "Classifier threshold:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_2c2931510812437899b50874520958b0",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": ".2f",
      "step": 1e-05,
      "style": "IPY_MODEL_702fc0ae0e9646e195974358ada8698e",
      "value": 0.5
     }
    },
    "f151a00a84a444a2ac3dde95d6449f76": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb777cd857e848febd846dabb6f351e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
